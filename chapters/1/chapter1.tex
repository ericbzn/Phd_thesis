% created on 30/03/2020
% @author : ebazan
\chapter{Introduction}

\section{Background and Motivation}
The objective of computer vision is to make machines capable of seeing and perceiving the world as we humans do. However, replicating the behavior of human vision on machines is a difficult work. Human vision is capable of performing tasks quite effortlessly and effectively, whereas for a machine, performing the same task can be quite challenging.

Along the history there have been numerous of studies which have tried to understand what is the nature of computation involved in visual tasks. Researchers from various fields of study such as biology, neuroscience and computer science have attempted to answer the question of how we might build the machines to be able to see and understand the world as we do.


\subsection{Computer Vision Milestone Works and History}
Computer vison is a rapidly evolving field. The three lines of research with which computer vision was born in the 1950s are replication of the eye, replication of the visual cortex, and replication of the rest of the brain. The joint work between psychologists and computer scientists led to the development of the Perceptron machine in 1957, the same year that marked the birth of the pixel.

By the 1960s, the problems of computer vision were linked to the representation of solid objects in 3D using simpler 2D figures. Such problems, along with other ideas such as connecting a camera to a computer to describe what he saw, motivated the creation of the MIT Artificial Intelligence Laboratory; After more than 50 years, this is an idea that continues to work.

Many of the vision algorithms that we know today, such as image edge extraction, line labialization, modeling, and object representation such as interconnections of smaller structures, optical flow, and motion estimation, were created in the 1970s. All of them driven by the first commercial applications of computer vision such as optical character recognition (OCR) and by the appearance of the first commercial camera.

The 80's was characterized by the appearance of studies based on more rigorous mathematical analysis and on quantitative aspects. Many non-linear image analysis algorithms such as mathematical morphology were generalized for grayscale and image functions.

Investigation of 3D reconstructions from projections or scenes from multiple images as well as camera calibration came in the 1990s. Along with this, there were advances in the field of stereo imaging and stereo multi-view techniques. In addition, variations of graph cutting algorithms were used for image segmentation thus enabling high-level interpretation of the coherent regions of an image.

By the 2000s, there was a vast range of applications and fields related to computer vision. Some of the most relevant topics from that time are face detection, brought to fame by Viola and Jones, Lowe's Invariant Scale Features Transform (SIFT) developed as well as the appearance of Neural Networks, which gave way to learning methods. This decade marked a breakthrough for computer vision thanks to the large number of annotated data sets that were accessible on the internet. With them, it was possible to propose increasingly complex challenges, which allowed comparing and quantifying computer vision techniques.

Techniques based on learning methods have become popular in the last decade. A milestone of this time is the large ImageNet visual database, created in 2009 for research on visual object recognition software. Since 2010, the project has organized an annual software competition, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software competes to correctly classify objects and scenes in the database. Until 2012, no program had been as relevant as AlexNet, a deep convolutional neural network (CNN) which surpassed the state of the art works of that time. The results of this competition undoubtedly marked the history of computer vision. In recent years, the study of CNNs has been exponential, giving way to novel systems such as Gernerative Adversarial Networks (GANs), which seek to make CNNs more robust.

Convolutionary neural networks not only became the choice of many researchers to carry out computer vision applications, large companies such as Google, Amazon, Facebook and Apple (GAFA) have promoted research and development in this field. A remarkable example is the DeepFace algorithm for facial recognition. The algorithm proposed by the Facebook researchers is capable of correctly identifying faces 97$\%$ of the time, this result being comparable to the detection that a human can do.

\subsection{Applications and Tasks}

The advancement of Computer Vision techniques has favored its use in a wide range of applications. The development has been outstanding in already traditional application areas such as multimedia, robotics or medicine. However, new areas of application continue to emerge such as augmented reality, automated driving, the Internet of Things (IoT) and the Industry 4.0, human-computer interaction and vision for the blind. Some less traditional areas where computer vision is increasingly present are astronomy, nanotechnology, new brain imaging techniques, among others.

While computer vision has outstripped the capabilities of human vision, computers have not completely replaced human personnel. For example, in the case of industrial vision systems tasks, say inspecting bottles or circuit boards on a production line, CV algorithms surpasses humans. However, in areas such as medical imaging, computer vision systems are only responsible for supplementing certain routine diagnoses that require a lot of time and experience from human doctors. This is largely related to the complexity of the task and the conditions under which the application is carried out. In the case of machine vision systems, working conditions are generally controlled, while in areas such as medicine, each patient image is different despite the fact that the acquisition system is the same.

Regardless of the application, computer vision systems must perform a number of tasks to achieve their end. Generally, these tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extracting real-world data to produce symbolic information, for example, in the form of decisions. Depending on the context, understanding images can mean transforming visual images (the sensor input) into descriptions of the world that can interact with other processes and provoke appropriate action. This compression can be seen as the crumbling of the symbolic information in the image using geometric, physical, statistical or theory of learning models.

More particularly, the tasks of computer vision can be grouped into four more or less well-defined processing problems: i) Recognition, a classic problem of computer vision which is responsible for determining whether an image contains any object, characteristic or exercise. Some variants of this problem are the classification, identification and detection of objects from which many specialized tasks emerge, for example, content-based image search, pose estimation, optical character recognition, reading of 2D codes, facial recognition, shape recognition, among others.
ii) Motion analysis, in which a sequence of images is processed to produce an estimate of the speed of one or more points of interest within a 3D image or scene. Some examples of this task are egomotion, object tracking, and optical flow.
iii) Scene reconstruction, which is a task related to the computation of a 3D model from one or more images of a scene.
iv) Image restoration, whose objective is to remove those imperfections of an image generated by disturbances such as sensor noise or motion blur. Generally this task is carried out in the pre-processing of the image before passing it to a more complex mink algorithm. An example in which this task is applied is inpainting.

Computer vision has become an integral technology in our daily and today is an exciting time to work on computer vision. Thanks to free access databases on the internet, computation capacity growth and, novel computer vision algorithms, machines can perform vision tasks faster and more efficiently than a human. In addition, access to these technologies has led to the development of novel applications, which favor not only the advancement of computer vision, but also favor society.  life.

\section{Scope of the Thesis}

Unmanned aerial vehicles (UAVs) or drones are these flying engines that are increasingly present in our lives. We can found them in various sectors, such as the military, commercial or civil, where they are able to carry out specific tasks quite efficiently. However, in most cases the development of such applications requires an expert pilot to control the aircraft. Commonly, the UAV  control is achieved with the use of conventional sensors, such as inertial sensors (IMUs) for orientation, and GPS for position. The combination of information from these sensors in a flight computer allow the drones to remain stable in the air. The drawback with the IMU is that suffers from bias error propagation due to the integral drift, while with the GPS signal is not always guaranteed. For example, in urban or indoor environments, the satellite signal is low or unexisting. 

A recurrent technique to enhance the position accuracy implies the data fusion of pressure, ultrasonic, radars and laser range-finders sensors \cite{Tomic.Schmid.ea:IRAM:2012}. The fusion of data can provide the advantages of each sensor; however, a major limitation of these complex systems is the flight time, parameter which is mainly linked to the total weight of the vehicle and the capacity of the battery. Therefore, the use of multiple sensors on board becomes expensive and impractical.

The capabilities of a drone are extended when some type of visual sensor is integrated. Contrariwise to other sensors such as Lidars, visual sensors are passive, lightweight and can acquire valuable information about the surrounding structures, including color and textures, and UAV self-motion. The addition of visual sensors to perceive the environment has been a recurring strategy that has made these aerial robots more manipulable, safer and even in some cases autonomous, that is, that the drone is capable of performing a task without the need for a human intervention. For this, the drone must be able to move without getting lost but above all, it must be able to detect and avoid potential obstacles on its way. 
Today one can use different visual sensors; such as monocular cameras \cite{Padhy.Xia.ea:TSC:2018}, stereo cameras \cite{Seitz.Curless.ea:CVPR:2006}, RGB-D cameras \cite{Huang.Bachrach.ea:RobR:2017}, fish-eye cameras \cite{Hrabar.Sukhatme:IROS:2004}, thermal \cite{Gaszczak.Breckon.ea:IRCV:2011}, among others. This wide range of sensors offers more options and flexibility to deal with the problems mentioned above. Whether in entertainment activities used as toys, or in commercial activities used as work tools, drones allow us to see the world and obtain information from another perspective, literally.


Therefore, the scope of this thesis is related to the computer vision methods for the detection of objects and the understanding of scenes within the framework of aerial vehicles and the possible applications and tasks carried out in complex outdoor environments. For this purpose, the algorithms developed in the thesis are based on traditional vision techniques, that is, non-deep learning techniques. In addition, the algorithms proposed in this document use both black and white and color as input images from low-cost monocular cameras.

\section{Problem Statement}
 
We can interpret the applications made with drones as missions. Generally, such missions involve three main moments: take-off, navigation, and landing. Of these three moments, navigation and landing are the stages in which visual information from on-board sensors and computer vision algorithms most frequently intervene.
Although the landing stage may have well-defined needs since it occurs at the end of the mission and there can be pre-designed elements, for example landing targets or landing platforms to help in the task, the problems related to computer vision applications for air vehicles are generated by the nature of the applications during the drone navigation. 

Drone missions are generally carried out in complex scenes that change as the vehicle moves through space. For example, a drone that delivers packages can start its route in a commercial area, where the scenes are mostly cluttered of halls and big open spaces such as parkings. Then, it could pass through rural areas, where the scenes can contain framlands or wooded areas. Finally, when the drone reaches the delivery point within an urban zone, the environment may contain houses, trees, electricity and telecommunications poles, etc. This results in overexposed and / or dark images due to considerable lighting changes and the precense of shadows. In addition to the lack of control of these conditions, we must also consider that the position and orientation of the camera with respect to a scene varies depending on the height and orientation of the vehicle. Therefore, the objects present in the images may have deformations. Finally, we must not forget that image acquisition is carried out by an on-board camera, which is generally not stabilized, in consequence, the images may be noisy or blurry.

Such problems prevent the existence of any global computer vision algorithm that is efficient in all or most of the situations described above. The most efficient algorithms to date are those based on neural network architectures and supervised learning techniques. However, these techniques have remarkable disadvantages that question their usability and applicability in real-life drone missions. 
From a practical and even economic point of view, there is a limit to the number of applications in which we can use supervised methods given the fact that we need a lot of annotated data. The collection and the correct labeling of data representative of a problem is true only for a small number of applications. 
The need of abundant information comes with high computational times required for model learning, which can range from a couple of hours to entire weeks. Of course, this variable can be minimized by increasing the computing power of our machines, however, today only those companies that have large computing infrastructures can afford to train models with a thousand of millions of parameters. 
This brings me to the next disadvantage of deep neural network based learning models: hyperparameters. Hyperparameters can be roughly divided into two categories, i) optimizer hyperparameters, which include learning rate, batch size, and number of epochs, and; ii) model-specific hyperparameters, including the number of hidden layers, the first hidden layer, and the number of layers. Choosing the appropriate hyperparameters plays a key role in the success of neural network architectures because they control the behavior of the learning algorithm, defining the structure of the network and how the network is trained. Although there are methods to optimize the choice of hyperparameters of a neural network, generally this is a heuristic process and its tuning is carried out for a specific problem. We cannot know the best value for a hyperparameter, it is possible to follow some rules based on experience, copy same values from some other problem or make the setting by trial and error.

 

\section{Objectives of the Thesis}
Traditional computer vision methods are (still) a reliable option to develop the object detection and recognition.

New deep learning techniques have need of traditional computer vision methods for overcome the lack of labeled data problem.

Generate a general image analysis framework for applications related with the autonomous navigation of aerial vehicles.

Test the framework for object detection in complex situations/scenarios such as those that may occur in applications such as autonomous navigation of air vehicles.

To compare qualitatively and quantitatively the detection / segmentation results of image objects with the most widely used algorithms in the literature based on deep learning techniques and with the most outstanding state-of-art methods based on traditional computer vision techniques.

There are hundreds of highly performing algorithms for image segmentation and object detection based on neural networks and artificial intelligence, however, these algorithms are in trouble when it comes to image analysis of complex scenarios or applications where there are no a database rich enough to do the learning process.


\section{General Literature Review}
In general, we can consider that each drone application is a mission that involves three main moments: take-off, navigation, and landing.   



\subsection{UAV Navigation}
Vision-based techniques for UAV navigation are classified into two groups according to the intention and its consequence result in i) localization and mapping, ii) obstacle avoidance.

\subsubsection{Localization and Mapping}
The so-called Simultaneous Localization and Mapping (SLAM) is a technique that estimates the local pose of a robot and builds a 3D model of its surroundings employing visual sensors. The Visual Odometry (VO) \cite{Scaramuzza.Fraundorfer:RAM:2011} is responsible of the robot motion estimation while the maps are built with occupancy grid algorithms \cite{Thrun.Bu:AI:1996}. Taking into account the image information used to perform a SLAM, we can classify the methods in two classes.

\textbf{Feature-based Methods} extract a set of image features (e.g., lines, points) in a sequence of images. To do so, the invariant feature detectors most commonly used are Harris \cite{Harris.Stephens:AVC:1988}, SIFT \cite{Lowe:ICCV:1999}, FAST \cite{Rosten.Drummond:ECCV:2006}, SURF \cite{Bay.Ess.ea:CVIU:2008}. After that, the algorithm performs a feature matching through an invariant feature descriptor. The last stage is to perform the motion estimation using the data of other sensors; a local optimization is optional. 

The Parallel Tracking and Mapping (PTAM) \cite{Klein.Murray:ISMAR:2007} is one of the first and most used methods for SLAM. It is a feature-based algorithm that achieves robustness through tracking and mapping hundreds of features. It is the base for other approaches such as the Multi-Camera Parallel Tracking Mapping (MCPTAM) \cite{Harmat.Trentini.ea:IROS:2015} that uses multiple cameras to build a 3D map and calculate the robot position. \\

\textbf{Direct-Based Methods} make use of the image intensity information to estimate the structure and the motion of the robot. Unlike the feature-based methods, the direct methods compare the entire image between them to make a scene reconstruction. The use of image intensity information permits using different theoretical frameworks for SLAM. The DTAM method \cite{Newcombe.Lovegrove.ea:ICCV:2011} uses the minimization of a global, spatially-regularized energy functional while \cite{Moulon.Monasse.ea:ACCV:2012} uses a contrario framework to carry out a structure from motion.
Those methods provide more visual information about the environment giving a more meaningful representation to the human eye. 

Some of the first direct approaches \cite{Jin.Favaro.ea:VC:2003}, \cite{Molton.Davison.ea:BMVC:2004} treated salient feature patches as observations of locally planar regions on 3D world surfaces. This approach allows being robust to images where exits areas with textures and small gradients \cite{Lovegrove.Davison.ea:IVS:2011} or to blur images caused for camera-defocus \cite{Newcombe.Lovegrove.ea:ICCV:2011}.\\

The use of SLAM techniques for UAV navigation presents remarkable advantages. Feature-based methods can use a wide variety of feature detectors which counts typically with an optimization stage that allows having fast algorithms. Direct-based methods have the advantage to be robust to images degradations; they can lead better with images with texture and blurred zones; besides, the map produced is of an acceptable resolution. An interesting fact is that the strengths of the first group of methods are the weak points of the second and vice versa. A method that tries to gather the benefits of both approaches is the Semi-direct Visual Odometry \cite{Forster.Pizzoli.ea:ICRA:2014}; however, in general, the SLAM methods works in indoor environments, where the illumination conditions are static or controlled.\\

\subsubsection{Obstacle Avoidance}
An indispensable feature to increase the autonomy of the UAV navigation is the detection and avoidance of obstacles. This capability is of great importance for achieving free collisions missions in both, indoor and outdoor environments. A recurrent solution, as we early mentioned, is the multi-sensor data fusion. In \cite{Gageik.Benz.ea:ACCESS:2015} present a platform using low-cost ultrasound and IR sensors; however, despite the obtained results, it utilizes several sensors to recovers environment information and yet, it does not get a perceptual representation of the scene due to the low resolution and perceptive capacity of the sensors. On the other hand, vision-based techniques for obstacle avoidance could identify obstacles and in some cases classify the found object \cite{Li.Ye.ea:IROS:2016}. 

Visual methods for avoidance of obstacles can be classified into two groups. The first, SLAM-based techniques, make use of the principles described in the last subsection. The 3D reconstruction provides accurate and sophisticated maps and allows the air vehicle to travel with more information about the environment. In \cite{Moreno-Armendariz.Calvo:ICMEAE:2014}, takes this advantage to develop an obstacle avoidance approach for static and dynamic obstacles. 

The second group is the flow-based methods which historically, were inspired by the navigation of insects such as bees \cite{Srinivasan.Gregory:PTBS:1992} or flies \cite{Franceschini.Ruffier.ea:InTech:2009}. Many insects in the wild identify obstacles through the intensity of light. During the flight, their eyes produce an optical flow that provides accurate spatial information. Currently, there are also works inspired by the behavior of the human eye \cite{Al-Kaff.Meng.ea:IVS:2016}. The technique measures the object size from the idea that objects in the robot's field of vision are larger as the obstacle is closer.\\

The techniques for obstacle detection and avoidance present interesting characteristics and ideas; however, its implementation is strongly linked to an application under certain conditions. Their use would involve a recalibration or readjustment of parameters and, given the conditions in which a drone can operate, it is necessary to have more general and non-supervised methods.

\section{Organization of the Document}
A new framework based on low-level image primitives that allows detection of objects in a totally unsupervised way.

A comparative study between the widely used image detection / segmentation techniques using deep learning architectures and traditional computer vision techniques.

In general terms, this thesis contributes to the debate between deep learning computer vision techniques and traditional computer vision techniques in the sense that it explores and attempts to solve highly complex problems without the need for an annotated database.


Throughout this research work I used different low-level image primitives as a source of information to develop a framework for object detection and recognition. The development process of this framework was evolutionary, where each time it was added a new primitive to enrich the representation of the image. Thus, my thesis laid out follows this evolutionary process of the research work: The first part of the thesis is devoted to the study of existing contours / edges in an image. I used this information for the detection of landing paths related with the autonomous landing drone task. The second part of this document is dedicated to the study of color information in an image. This part serves as an introduction to the third part of the thesis, dedicated to the study of textures generated by the lighting changes. These two parts engage in an image search system from a query image that shows the importance of color spaces and texture in image analysis. Part number four brings together the theory used in the preceding parts to create a framework that analyzes the colors of the textures as well as the textures generated by lighting changes. In this part we show the high-level primitives that can be obtained from the proposed image representation and present a comparison between various clustering methods obtain a totally unsupervised image segmentation.


KEY PHRASE: \textbf{Perceptual image information for object detection and segmentation}



