# PHD WRITING TEMPLATE

------

## *Thesis template Draw N°1*

### <u>**Title:** Vision Methods for Navigation of Unmanned Aerial Vehicles (UAVs)</u>

### **Aims and objectives** (If someone asked you what is my thesis about, this is what I said)

*<u>Aim:</u>* Propose computer vision algorithms to aid autonomous drone navigation.

*<u>Scopes:</u>*  

- Monochrome and/or RGB cameras as sensors for data acquisition

- Use of classic methods of image analysis for segmentation/ detection of objects

*<u>Gap:</u>* 

- Insufficiency of computer vision algorithms robust enough to the natural disturbances generated in missions with drones.

*<u>Main argument:</u>*

There are hundreds of highly performing algorithms for image segmentation and object detection based on neural networks and artificial intelligence, however, these algorithms are in trouble when it comes to image analysis of complex scenarios or applications where there are no a database rich enough to do the learning process.

*<u>Contribution:</u>*

A new vision framework able to provide aid in the automated decision chain of UAVs in complex scenarios.

------

## *Thesis template Draw N°2*

### <u>**Title:** Traditional Computer Vision Methods for Unmanned Aerial Vehicles (UAVs) Applications</u>

### **Aims and objectives** (If someone asked you what is my thesis about, this is what I said)

*<u>Aim:</u>* 

Propose computer vision algorithms for object detection/segmentation using classic machine learning approaches and low-level features for the image analysis.

*<u>Scopes:</u>*  

- Vision-based methods to aid autonomous drone navigation.

- Monochrome and/or RGB cameras as sensors for data acquisition.

*<u>Gaps:</u>* 

- Abandonment of classical methods for image analysis.
- Lack of unsupervised methods in image analysis for applications such as the drone navigation.
- Insufficiency of computer vision algorithms robust enough to the natural disturbances generated in missions with drones.

*<u>Main argument:</u>*

There are hundreds of highly performing algorithms for image segmentation and object detection based on neural networks and artificial intelligence, however, these algorithms are in trouble when it comes to image analysis of complex scenarios or applications where there are no a database rich enough to do the learning process.

*<u>Contribution:</u>*

A new vision framework able to perform the detection/segmentation of objects in an unsupervised way based on low-level image features and classic image analysis theory. This framework is tested on applications/situations developed in complex scenarios related with providing aid in the automated decision chain of UAV’s navigation.

------

### **1. Acknowledges**

------

### **2. Abstract**

------

### **3. Introduction**

*<u>What is the thesis problem statement?</u>*

Problems linked to computer vision applications for air vehicles are generated by the nature of the object detection task for autonomous navigation. Such missions are generally carried out in complex scenes that change as the vehicle moves through space. For example, a drone that delivers packages can start its route in a commercial area, passing through rural areas until the order is delivered within an urban area. In this case we find situations where visual information and objects suffer deformations due to changes in height and direction of the vehicle. In addition, the images captured by the drone can present strong changes in lighting and the presence of shadows that disturb the perception of a scene.

*<u>What do I (not) hope to achieve?</u>*

1. Generate a general image analysis framework for applications related with the autonomous navigation of aerial vehicles.

2. Test the framework for object detection in complex situations/scenarios such as those that may occur in applications such as autonomous navigation of air vehicles.

3. To compare qualitatively and quantitatively the detection / segmentation results of image objects with the most widely used algorithms in the literature based on deep learning techniques and with the most outstanding state-of-art methods based on traditional computer vision techniques.

*<u>What are the research questions and hypotheses?</u>*

1. Traditional computer vision methods are (still) a reliable option to develop the object detection and recognition.

2. New deep learning techniques have need of traditional computer vision methods for overcome the lack of labeled data problem.

*<u>What is my contribution to the field?</u>*

1. A new framework based on low-level image primitives that allows detection of objects in a totally unsupervised way.

2. A comparative study between the widely used image detection / segmentation techniques using deep learning architectures and traditional computer vision techniques.

3. In general terms, this thesis contributes to the debate between deep learning computer vision techniques and traditional computer vision techniques in the sense that it explores and attempts to solve highly complex problems without the need for an annotated database.

*<u>How is my thesis laid out?</u>*

Throughout this research work I used different low-level image primitives as a source of information to develop a framework for object detection and recognition. The development process of this framework was evolutionary, where each time it was added a new primitive to enrich the representation of the image. Thus, my thesis laid out follows this evolutionary process of the research work: The first part of the thesis is devoted to the study of existing contours / edges in an image. I used this information for the detection of landing paths related with the autonomous landing drone task. The second part of this document is dedicated to the study of color information in an image. This part serves as an introduction to the third part of the thesis, dedicated to the study of textures generated by the lighting changes. These two parts engage in an image search system from a query image that shows the importance of color spaces and texture in image analysis. Part number four brings together the theory used in the preceding parts to create a framework that analyzes the colors of the textures as well as the textures generated by lighting changes. In this part we show the high-level primitives that can be obtained from the proposed image representation and present a comparison between various clustering methods obtain a totally unsupervised image segmentation.

**Introduction writing**

<u>What is Computer Vision?</u>

*Situation:* Computer Vision (CV) is about making machines capable of seeing and perceiving the world as humans do.

*Problem/Question:* Replicating the behavior of human vision on machines is a difficult task. The tasks that human vision performs  quite effortlessly and effectively, for a machine could represent a complex task.

*Solution:* There have been a large number of studies to date which have tried to understad what is the nature of computation involved in visual tasks. Researchers from various fields of study such as biology, neuroscience and computer science have attempted to answer the question of how we might build the machines to be able to see and understand the world as we do.

<u>Timelines and Milestones</u>

*Situation:* Computer vison is a rapidly evolving field. 

*Problem/Question:* What are the milestone works in CV field? To understand where we are today, it is important to know the history and the key milestones that has made this field grow.

*Solution/Answer:* 

The three lines of research with which computer vision was born in the 1950s are replication of the eye, replication of the visual cortex, and replication of the rest of the brain. The joint work between psychologists and computer scientists led to the development of the Perceptron machine in 1957, the same year that marked the birth of the pixel.

By the 1960s, the problems of computer vision were linked to the representation of solid objects in 3D using simpler 2D figures. Such problems, along with other ideas such as connecting a camera to a computer to describe what he saw, motivated the creation of the MIT Artificial Intelligence Laboratory; After more than 50 years, this is an idea that continues to work.

Many of the vision algorithms that we know today, such as image edge extraction, line labialization, modeling, and object representation such as interconnections of smaller structures, optical flow, and motion estimation, were created in the 1970s. All of them driven by the first commercial applications of computer vision such as optical character recognition (OCR) and by the appearance of the first commercial camera.

The 80's can be characterized by the appearance of studies based on more rigorous mathematical analysis and on quantitative aspects. Many non-linear image analysis algorithms such as mathematical morphology were generalized for grayscale and image functions.

Investigation of 3D reconstructions from projections or scenes from multiple images as well as camera calibration came in the 1990s. Along with this, there were advances in the field of stereo imaging and stereo multi-view techniques. . In addition, variations of graph cutting algorithms were used for image segmentation thus enabling high-level interpretation of the coherent regions of an image.

By the 2000s, there was a vast range of applications and fields related to computer vision. Some of the most relevant topics from that time are face detection, brought to fame by Viola and Jones, Lowe's Invariant Scale Features Transform (SIFT) developed as well as the appearance of Neural Networks, which gave way to learning methods. This decade marked a breakthrough for computer vision thanks to the large number of annotated data sets that were accessible on the internet. With them, it was possible to propose increasingly complex challenges, which allowed comparing and quantifying computer vision techniques.

Techniques based on learning methods have become popular in the last decade. A milestone of this time is the large ImageNet visual database, created in 2009 for research on visual object recognition software. Since 2010, the project has organized an annual software competition, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software competes to correctly classify objects and scenes in the database. Until 2012, no program had been as relevant as AlexNet, a deep convolutional neural network which surpassed the state of the art works of that time. The results of this competition undoubtedly marked the history of computer vision. In recent years, the study of CNNs has been exponential, giving way to novel systems such as GANs, which seek to make CNNs more robust.

Convolutionary neural networks not only became the choice of many researchers to carry out computer vision applications, large companies such as Google, Amazon, Facebook and Apple (GAFA) have promoted research and development in this field. A remarkable example is the DeepFace algorithm for facial recognition. The algorithm proposed by the Facebook researchers is capable of correctly identifying faces 97% of the time, this result being comparable to the detection that a human can do.

<u>Applications and Taks</u>

*Situation:* 

The advancement of Computer Vision techniques has favored its use in a wide range of applications. The development has been outstanding in already traditional application areas such as multimedia, robotics or medicine. However, new areas of application continue to emerge such as augmented reality, automated driving, the Internet of Things and Industry 4.0, human-computer interaction and vision for the blind. Some less traditional areas where Computer Vision is increasingly present are astronomy, nanotechnology, new brain imaging techniques, among others.

While computer vision has outstripped the capabilities of human vision, computers have not completely replaced human personnel. For example, in the case of industrial vision systems tasks, say inspecting bottles or circuit boards on a production line, computer vision surpasses humans. However, in areas such as medical imaging, computer vision systems are only responsible for supplementing certain routine diagnoses that require a lot of time and experience from human doctors. This is largely related to the complexity of the task and the conditions under which the application is carried out. In the case of machine vision systems, working conditions are generally controlled, while in areas such as medicine, each patient image is different despite the fact that the acquisition system is the same.

Regardless of the application, computer vision systems must perform a number of tasks to achieve their end. Generally, these tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extracting real-world data to produce symbolic information, for example, in the form of decisions. Depending on the context, understanding images can mean transforming visual images (the sensor input) into descriptions of the world that can interact with other processes and provoke appropriate action. This compression can be seen as the crumbling of the symbolic information in the image using geometric, physical, statistical or theory of learning models.

More particularly, the tasks of computer vision can be grouped into four more or less well-defined processing problems: Recognition, a classic problem of computer vision which is responsible for determining whether an image contains any object, characteristic or exercise. Some variants of this problem are the classification, identification and detection of objects from which many specialized tasks emerge, for example, content-based image search, pose estimation, optical character recognition, reading of 2D codes, facial recognition, shape recognition, among others.

Motion analysis, in which a sequence of images is processed to produce an estimate of the speed of one or more points of interest within a 3D image or scene. Some examples of this task are egomotion, object tracking, and optical flow.

The reconstruction of scenes, which is a task related to the computation of a 3D model from one or more images of a scene.

The restoration of images, whose objective is to remove those imperfections of an image generated by disturbances such as sensor noise or motion blur. Generally this task is carried out in the pre-processing of the image before passing it to a more complex mink algorithm. An example in which this task is applied is inpainting

Today is an exciting time to work on computer vision. Thanks to free access databases on the internet, novel computer vision algorithms and increased computing power, machines can perform vision tasks faster and more efficiently than a human. In addition, access to these technologies has led to the development of novel applications, which favor not only the advancement of computer vision, but also favor society. Computer vision has become an integral technology in our daily life.

<u>Scope of the thesis</u>

Unmanned aerial vehicles (UAVs) or drones are these flying machines that are increasingly present in our lives. The combination of different sensors of the inertial power station, the GPS and the flight computer allow the drones to remain stable in the air. However, the capabilities of a drone are extended when some type of visual sensor is integrated. Whether in entertainment activities used as toys, or in commercial activities used as work tools, drones allow us to see the world and obtain information from another perspective, literally.

However, a major limitation of these complex systems is the autonomy time. This parameter is mainly linked to the weight of the vehicle, given by the different components on board the aircraft and the capacity of the battery, which also influences the final weight of the equipment. In this sense, it is important to privilege those sensors that provide information from the outside world for navigation but also light and low energy consumption.

Taking this into account and remembering the areas of application and tasks of computer vision mentioned in the previous section, this thesis work will focus on those related to drones and object detection and scene comprehension tasks. Regarding the techniques and methods of the developed vision algorithms, this thesis exploits the now called traditional methods of computer vision, that is, those algorithms not based on neural network architectures and mainly on unsupervised methods.

<u>Problem Statement</u>

While drones are useful in a wide variety of applications, whether commercial, civil or military, in most cases an expert pilot is required to control the aircraft. The addition of external sensors to perceive the environment has been a recurring strategy that has made these aerial robots more manipulable, safer and even in some cases autonomous, that is, a drone is capable of performing a task without the need for a human intervention. For this, the drone must be able to move without getting lost but above all, it must be able to avoid potential obstacles on the way.

In general, we can consider that each drone application is a mission that involves three main moments:



We can interpret the applications made with drones as missions. Generally, such missions involve three main moments: clearance, navigation, and landing. Of these three moments, navigation and landing are the stages in which visual information from on-board sensors and computer vision algorithms most frequently intervene.

Problems linked to computer vision applications for air vehicles are generated by the nature of the applications. Such missions are generally carried out in complex scenes that change as the vehicle moves through space. For example, a drone that delivers packages can start its route in a commercial area, where the scenes are mostly cluttered of halls and big open spaces such as parkings. Then, it could pass through rural areas, where the scenes can contain framlands or wooded areas. Finally, the drone reaches the delivery point within an urban area, where the environment may contain houses, trees, electricity and telecommunications poles, etc. This results in overexposed and / or dark images due to considerable lighting changes and shadows.

In addition to the scenes and conditions are not constant or controllable throughout a mission with drones, we must also consider that the position and orientation of the camera varies depending on the height and orientation of the vehicle. Therefore, the objects present in the images may have deformations. Finally, we must not forget that image acquisition is carried out by an on-board camera, which is generally not stabilized. Therefore, the images may be noisy or blurry.